#################
# General Setting
#################
# Experiment name, if the experiment name has the word debug, it will enter debug mode
name: BaseModel_res18_imageNet_1000base_wandb_001
# The type of model used, usually the class name of the model defined in the `methods` directory
model_type: F2MModel
# gpu
gpu: '1'
# Random seed
manual_seed: 1997
Random: false

#################################
# The settings for the dataset and data loader
#################################
transformer_agu:
  - type: RandomResizedCrop
    size: 224
    #size: 84
#  - type: ColorJitter
#    brightness: 0.4
#    contrast: 0.4
#    saturation: 0.4
  - type: RandomHorizontalFlip
  - type: ToTensor
  - type: Normalize
    mean: !!python/tuple [0.485, 0.456, 0.406]
    std: !!python/tuple [0.229, 0.224, 0.225]

transformer:
  - type: Resize
    size: 256
    #size: 92
  - type: CenterCrop
    size: 224
    #size: 84
  - type: ToTensor
  - type: Normalize
    mean: !!python/tuple [ 0.485, 0.456, 0.406 ]
    std: !!python/tuple [ 0.229, 0.224, 0.225 ]

datasets:
  # The important information of dataset
  train:
    name: imagenet
    type: NormalDataset
    total_classes: 1000
    dataroot: /workspace/imagenet/imagenet/train
    aug: true
    # Number of processes read by the data loader per GPU
    num_worker_per_gpu: 16
    # Batch size
    batch_size_base_classes: 1024
    #batch_size: 8
    pin_memory: true
    pre_load: false

    sampler:
      type: ~
      #type: TaskSampler
      #num_samples: 8

  val:
    name: imagenet
    type: NormalDataset
    total_classes: 1000
    dataroot: /workspace/imagenet/imagenet/val
    aug: false
    pin_memory: true
    pre_load: false
    # Number of processes read by the data loader per GPU
    num_worker_per_gpu: 16

    sampler:
      type: ~
      #type: TaskSampler
      #num_samples: 8

#####################
# The following is the setup of the network structure
#####################
network_g:
  type: Resnet18_softmax
  Embed_dim: 512
  pretrained: False
  norm: false
  num_classes: 1000
  adopt_classifier: true
  addition_net: false
  flatten: true
  cut_at_pooling: false

######################
## 以下为网络结构的设置
######################
#network_g:
#  type: Resnet18_softmax
#  Embed_dim: 512
#  pretrained: false
#  norm: false
#  num_classes: 60
#  adopt_classifier: true
#  addition_net: false
#  flatten: true
#  cut_at_pooling: false

######################################
# The following are the paths
######################################
path:
  # Pre-trained model path, need models ending in pth
  #pretrain_model_g: ./exp/ImageNet_bases1000/CFRPModel_res18_ImageNet_{DB4_u3.0}{sfc8:0.0002}{std2.0}{PTFix:1.0}{rdt2}{BZ4_64}{1997}_1000base_wandb_001/models/best_net_latest.pth
  # Whether to load pretrained models strictly, that is the corresponding parameter names should be the same
  pretrain_model_g: ~
  #pretrain_model_g:  ./pretrain/resnet18-5c106cde.pth
  #pretrain_model_g: ./exp/miniImageNet_bases60/CFRModel_res18_miniImageNet_small_60base_wandb_001/models/best_net_latest.pth
  strict_load: false
  #
  resume_state: ~

#################
# The following is the training setup
#################
train:
  # Optimizer
  optim_g:
    # The type of optimizer
    type: SGD
    ##### The following properties are flexible and have different settings depending on the optimizer
    # learning rate
    #lr: !!float 1e-3
    lr: !!float 1e-1
    #lr_cf: !!float 1e-2
    weight_decay: !!float 5e-4
    # momentum
    momentum: !!float 0.9

  # The setting of scheduler
  scheduler:
    # The type of scheduler
    type: MultiStepLR
    #### The following properties are flexible, depending on the learning rate Scheduler has different settings
    # milestones
    #milestones: [ !!python/object/apply:eval [ 600 * 3 ],
    #              !!python/object/apply:eval [ 900 * 3 ]]
    milestones: [37500,75000,112500]
    # gama
    gamma: 0.1

  #random_noise:
  #  distribution:
  #    type: DiscreteBeta
  #  type: suffix_conv_weight
  #  num_layers: 8
  #  low: 0.1
  #  high: 3.0
  #  reduction_factor: 4
  #  bound_value: 0.01
  #  random_times: 2

  # The number of epoch
  epoch: 100
  original_loss: false
  # The number of warm up iteration, if -1, means no warm up
  warmup_iter: -1  # no warm up
  # number of base classes
  bases: 1000
  # number of tasks for incremental few-shot learning
  tasks: 1
  std_w: 2.0

  proto_loss:
    type: ProtoFixLoss
    shots: 8
    w: 1.0

#######################
# The following are the settings for Validation
#######################
val:
  # The frequency of validation
  val_freq: 10000
  debug_val_freq: 10000

#  random_noise:
#    distribution:
#      type: DiscreteUniform2
#    type: pre_bn
#    num_layers: 1
#    reduction_factor: 4
#    bound_value: 0.02
#    random_times: 1

####################
# The following are the settings for Logging
####################
logger:
  # Frequency of loggers printed on the screen according to iteration
  print_freq: 100
  # 保存checkpoint的频率
  save_checkpoint_freq: 20000
  # Whether to use tensorboard logger

  use_tb_logger: true
  # Whether to use wandb logger
  wandb:
    # The default is None, i.e. wandb is not used.
    project: FSIL-imagenet-1000bases-noBuffer
    # If it is resume, you can enter the last wandb id, then the log can be connected
    resume_id: ~
